# Daily Thought (2019.4.8 - 2019.4.13)
**Do More Thinking!** ♈ 

**Ask More Questions!** ♑

**Nothing But the Intuition!** ♐

### 1. 对resnet本质的一些思考
FROM: https://zhuanlan.zhihu.com/p/60668529

**非线性激活层**

目的：引入了非线性，让模型具有更强的拟合能力

副作用：ReLU会造成的低维数据的坍塌（collapse）

解释：低维度的feature在通过ReLU的时候，这个feature会像塌方了一样，有一部分被毁掉了，或者说失去了

具体验证方式就是我们对于一个feature,经过一个映射T,映射到低维的embedding space，然后将这个低维embedding经过RELU，然后在通过同样的T的逆变换，将其映射到原来的featrue的空间

会发现像下面所展示的一样这时候的feature已经发生了很大的变化

![](__pics/resnet_known_1.jpg)

ReLU这个东西，其实就是一个滤波器，只不过这个滤波器的作用域不是信号处理中的频域，而是特征域。那么滤波器又有什么作用呢？维度压缩

也就是降维，我们有m个feature被送入ReLU层，过滤剩下n个（n<m）

**低维数据流经非线性激活层会发生坍塌（信息丢失），而高维数据就不会**

维度低的feature，分布到ReLU的激活带上的概率小，因此经过后信息丢失严重，甚至可能完全丢失。而维度高的feature，分布到ReLU的激活带上的概率大，虽然可能也会有信息的部分丢失，但是无伤大雅，大部分的信息仍然得以保留。

因为维度低的feature信息冗余度较低，所以一旦损失一部分后果较严重，然而对于高维度的信息损失基本大部分都是需要淘汰的，经过学习优胜劣汰下来的有用信息保留度会高。

无法激活的值，当信息无法流过ReLU时，该神经元的输出就会变为0。而在反向传播的过程中，ReLU对0值的梯度为0，即发生了梯度消失。

对于一个M维的数据，我们可以将其看成是在M维空间中的一个M维流形（manifold）。而其中的有用信息，就是在该M维空间中的一个子空间（子空间的维度记为N维，N<=M）中的一个N维流形。非线性激活层相当于压缩了这个M维空间的维度（还记得前面提过的维度压缩吗？）。若是该M维空间中的M维流形本来就不含有冗余信息（M=N），那么再对其进行维度压缩，必然导致信息的丢失。

而维度低的数据其实就是这么一种情况：其信息的冗余度高的可能性本来就低，如果强行对其进行非线性激活（维度压缩），则很有可能丢失掉有用信息，甚至丢失掉全部信息（输出为全0）。

与非线性激活层不同的是，线性激活层并不压缩特征空间的维度。

**使用激活层原则**
- 1. 对含有冗余信息的数据使用非线性激活（如ReLU），对不含冗余信息的数据使用线性激活（如一些线性变换）。
- 2. 两种类型的激活交替灵活使用，以同时兼顾非线性和信息的完整性。
- 3. 由于冗余信息和非冗余信息所携带的有用信息是一样多的，因此在设计网络时，对内存消耗大的结构最好是用在非冗余信息上。

**ResNet就是满足这样原则的设计**

降低数据中信息的冗余度。

![](__pics/resnet_known_2.png)

x分支：对非冗余信息采用了线性激活（通过skip connection获得无冗余的identity部分）

参差分支：然后对冗余信息采用了非线性激活（通过ReLU对identity之外的其余部分进行信息提取/过滤，提取出的有用信息即是残差）。

**从数据中拿掉了非冗余信息的identity部分，会导致余下部分的信息冗余度变高。这就像从接近饱和的溶液中移走了一部分溶质，会使得剩下的溶液的饱和度降低，一个道理。**

**Resnet它带来的好处：**

- 1. 由于identity之外的其余部分的信息冗余度较高，因此在对其使用ReLU进行非线性激活时，丢失的有用信息也会较少，ReLU层输出为0的可能性也会较低。这就降低了在反向传播时ReLU的梯度消失的概率，从而便于网络的加深，以大大地发挥深度网络的潜能。
- 2. 特征复用能加快模型的学习速度，因为参数的优化收敛得快（从identity的基础上直接学习残差，总比从头学习全部来得快）。

**最后是两个小tips：**
- 1. 如果一个信息可以完整地流过一个非线性激活层，则这个非线性激活层对于这个信息而言，相当于仅仅作了一个线性激活。
- 2. 解决由非线性激活导致的反向传播梯度消失的窍门，就是要提高进行非线性激活的信息的冗余度。

### 2. 对注意力机制 本质的一些思考
FROM：https://www.zhihu.com/question/318731473/answer/641021746

注意力实际是对信息进行加权，是引入了一个度量或者内积，所以注意力的主要作用是对重要的信息进行加权，从而防止重要信息被噪声掩盖。也许通过这种加权可以保证重要的信息在网络中更有效的传递。

### 3. 对cycleGAN中的对偶损失的理解
为什么要使用cycleGAN，加一个这样的cycle consistency loss

**原因**：

本质上主要是为了解决**没有成对数据**的问题，也就是说例如风格迁移问题，如果domain A有一张图片，想要转成到另一种风格也就是转到domain B，那么其实domain B 是没有对应的ground truth的。

所以怎么办呢？通过这样一种方式，转到domain B之后的监督是看看是否与这个domain融为一体，但是为了保证内容没有问题，所以同时再转回去，做一个**重构一致损失**。

这里对于cycleGAN其实有些问题虽然也是没有成对数据但是不一定适合，就是因为domain A与domain B的数据量是不对等的，也就是domain B的某个样例可能有多个domain A可能性。

所以cycleGAN的一个**缺点**：虽然保证了一对一，但是也限制了多样性

**事实上，B->A->B，不一定需要恢复到完全一样的B**
